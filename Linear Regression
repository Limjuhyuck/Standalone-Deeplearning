""""Linear Regression"""""

X = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Y = [1, 1, 2, 4, 5, 7, 8, 9, 9, 10]

import matplotlib.pyplot as plt  #X, Y 데이터를 2차원 평면에 시각화
#plt.scatter(X, Y)
#plt.show()

"""""Model define"""""
class H():

    def __init__(self,w): # w를 input으로 받아서 모델 만듦. w는 학습을 통해 최적화될 값
        self.w = w

    def forward(self, x):
        return self.w * x # x가 input으로 들어왔을 때 예측되는 y를 return
h = H(4) # f(x) = 4x 라는 모델
pred_y = h.forward(5)  #5를 h model의 input 으로
print('value of f(5) : ',pred_y)          #20 : 5가 input으로 들어왔을 때 출력되는 y
print('value of w : ',h.w)             #4 : model h 내부의 parameter

"""""Cost Function Define(Loss Function)"""""
def cost(h, X, Y): # Error를 확인하는 Cost function
    error = 0
    for i in range(len(X)):
        error += (h.forward(X[i]) - Y[i])**2 # 예측했던 선형방정식과 실제 값의 차이의 제곱
    error = error / len(X) # error의 평균
    return error # error 출력

def better_cost(pred_y, true_y): # 예측한 y 값들의 리스트와 실제 y 값들의 리스트를 받아서 계산
    error = 0
    for i in range(len(X)):
        error += (pred_y[i] - true_y[i])**2
        error = error / len(X)
        return error

# 다양한 w에 따라 cost 값 어떻게 변하는 지 확인
list_w = []
list_c = []
for i in range(-20, 20):
    w = i * 0.5
    h = H(w)
    c = cost(h, X, Y)
    list_w.append(w)
    list_c.append(c)

print(list_w) # w에 들어간 값들 확인
print(list_c) # c에 들어간 값들 확인

plt.figure(figsize=(10, 5)) # 창 크기
plt.xlabel('w')
plt.ylabel('cost')
plt.scatter(list_w, list_c, s=3)
plt.show()

def cal_grad(w, cost): # 특정 w=4인 지점과 w = 4 + eps 인 지점에서의
    h = H(w)           # cost 값을 각각 구해서 두 값의 차이를 이용하여
    cost1 = cost(h, X, Y) # gradient를 구하는 방법
    eps = 0.00001
    h = H(w+eps)  # w에서 eps 만큼 떨어진 지점에서의 cost 값도 구함
    cost2 = cost(h, X, Y)
    dcost = cost2 - cost1  # w 에서의 cost 와 w + esp 에서의 cost 차이
    dw = eps
    grad = dcost / dw
    return grad, (cost1+cost2)*0.5

def cal_grad2(w, cost): # 편미분한 공식을 이용해서 gradient 근사하는 방법
    h = H(w)
    grad = 0
    for i in range(len(X)):
        grad += 2 * (h.forward(X[i]) - Y[i]) * X[i]
    grad = grad / len(X)
    c = cost(h, X, Y)
    return grad, c

#  w = 1.4라는 지점에서 모델을 초기화하고 위의 두 방법을 사용했을 때 각각의 결과 비교
w1 = 1.4
w2 = 1.4
lr = 0.01

list_w1 = []
list_c1 = []
list_w2 = []
list_c2 = []

for i in range(100): # 100번의 iteration
    grad, mean_cost = cal_grad(w1, cost)  # 첫번째 방법
    grad2, mean_cost2 = cal_grad2(w2, cost)  # 두번째 방법

    w1 -= lr * grad # 기울기가 나온 반대방향으로 가야 0에 가까워지므로 -=
    w2 -= lr * grad2
    list_w1.append(w1)
    list_w2.append(w2)
    list_c1.append(mean_cost)
    list_c2.append(mean_cost2)

print(w1, mean_cost, w2, mean_cost2) # 결과 출력 : 둘 다 1.21 정도로 비슷한 결과
plt.scatter(list_w1, list_c1, label='analytic', marker='*') # *로 그래프에 점 표시
plt.scatter(list_w2, list_c2, label='formula') # 그냥 점으로 표시
plt.legend()
plt.show()

# 편미분 방식을 이용하되 w가 서로 다른 지점으로 초기화 되었을 경우
w1 = 1.4
w2 = 1.1 # w1과는 다르게 1.1에서 초기화 / 다른 값들로 바꿔봐도 1.21 정도로 나옴
lr = 0.01

list_w1 = []
list_c1 = []
list_w2 = []
list_c2 = []

for i in range(100): # 100번의 iteration
    grad, mean_cost = cal_grad2(w1, cost)
    grad2, mean_cost2 = cal_grad2(w2, cost)

    w1 -= lr * grad
    w2 -= lr * grad2
    list_w1.append(w1)
    list_w2.append(w2)
    list_c1.append(mean_cost)
    list_c2.append(mean_cost2)

print(w1, mean_cost, w2, mean_cost2) # 결과 출력 : 둘 다 1.21로 비슷한 결과
plt.scatter(list_w1, list_c1, label='start from w=1.4', marker='*')
plt.scatter(list_w2, list_c2, label='start from w=1.1')
plt.legend()
plt.show()
